{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1caca8b",
   "metadata": {},
   "source": [
    "# Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ac68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from random import choices\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import DutchStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf63695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://test:test@rs.qug52es.mongodb.net/?retryWrites=true&w=majority\", connectTimeoutMS=30000, socketTimeoutMS=None, connect=False, maxPoolsize=1)\n",
    "db = client.get_database('RS')\n",
    "\n",
    "def find_all_interactions_history(json_data):\n",
    "    # Get collection\n",
    "    records = db.interactions\n",
    "    result = records.find({'user_id': json_data[\"user_id\"]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0185bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = [\n",
    "    \"aan\",\n",
    "    \"aangaande\",\n",
    "    \"aangezien\",\n",
    "    \"achte\",\n",
    "    \"achter\",\n",
    "    \"achterna\",\n",
    "    \"af\",\n",
    "    \"afgelopen\",\n",
    "    \"al\",\n",
    "    \"aldaar\",\n",
    "    \"aldus\",\n",
    "    \"alhoewel\",\n",
    "    \"alias\",\n",
    "    \"alle\",\n",
    "    \"allebei\",\n",
    "    \"alleen\",\n",
    "    \"alles\",\n",
    "    \"als\",\n",
    "    \"alsnog\",\n",
    "    \"altijd\",\n",
    "    \"altoos\",\n",
    "    \"ander\",\n",
    "    \"andere\",\n",
    "    \"anders\",\n",
    "    \"anderszins\",\n",
    "    \"beetje\",\n",
    "    \"behalve\",\n",
    "    \"behoudens\",\n",
    "    \"beide\",\n",
    "    \"beiden\",\n",
    "    \"ben\",\n",
    "    \"beneden\",\n",
    "    \"bent\",\n",
    "    \"bepaald\",\n",
    "    \"betreffende\",\n",
    "    \"bij\",\n",
    "    \"bijna\",\n",
    "    \"bijv\",\n",
    "    \"binnen\",\n",
    "    \"binnenin\",\n",
    "    \"blijkbaar\",\n",
    "    \"blijken\",\n",
    "    \"boven\",\n",
    "    \"bovenal\",\n",
    "    \"bovendien\",\n",
    "    \"bovengenoemd\",\n",
    "    \"bovenstaand\",\n",
    "    \"bovenvermeld\",\n",
    "    \"buiten\",\n",
    "    \"bv\",\n",
    "    \"daar\",\n",
    "    \"daardoor\",\n",
    "    \"daarheen\",\n",
    "    \"daarin\",\n",
    "    \"daarna\",\n",
    "    \"daarnet\",\n",
    "    \"daarom\",\n",
    "    \"daarop\",\n",
    "    \"daaruit\",\n",
    "    \"daarvanlangs\",\n",
    "    \"dan\",\n",
    "    \"dat\",\n",
    "    \"de\",\n",
    "    \"deden\",\n",
    "    \"deed\",\n",
    "    \"der\",\n",
    "    \"derde\",\n",
    "    \"derhalve\",\n",
    "    \"dertig\",\n",
    "    \"deze\",\n",
    "    \"dhr\",\n",
    "    \"die\",\n",
    "    \"dikwijls\",\n",
    "    \"dit\",\n",
    "    \"doch\",\n",
    "    \"doe\",\n",
    "    \"doen\",\n",
    "    \"doet\",\n",
    "    \"door\",\n",
    "    \"doorgaand\",\n",
    "    \"drie\",\n",
    "    \"duizend\",\n",
    "    \"dus\",\n",
    "    \"echter\",\n",
    "    \"een\",\n",
    "    \"eens\",\n",
    "    \"eer\",\n",
    "    \"eerdat\",\n",
    "    \"eerder\",\n",
    "    \"eerlang\",\n",
    "    \"eerst\",\n",
    "    \"eerste\",\n",
    "    \"eigen\",\n",
    "    \"eigenlijk\",\n",
    "    \"elk\",\n",
    "    \"elke\",\n",
    "    \"en\",\n",
    "    \"enig\",\n",
    "    \"enige\",\n",
    "    \"enigszins\",\n",
    "    \"enkel\",\n",
    "    \"er\",\n",
    "    \"erdoor\",\n",
    "    \"erg\",\n",
    "    \"ergens\",\n",
    "    \"etc\",\n",
    "    \"etcetera\",\n",
    "    \"even\",\n",
    "    \"eveneens\",\n",
    "    \"evenwel\",\n",
    "    \"gauw\",\n",
    "    \"ge\",\n",
    "    \"gedurende\",\n",
    "    \"geen\",\n",
    "    \"gehad\",\n",
    "    \"gekund\",\n",
    "    \"geleden\",\n",
    "    \"gelijk\",\n",
    "    \"gemoeten\",\n",
    "    \"gemogen\",\n",
    "    \"genoeg\",\n",
    "    \"geweest\",\n",
    "    \"gewoon\",\n",
    "    \"gewoonweg\",\n",
    "    \"haar\",\n",
    "    \"haarzelf\",\n",
    "    \"had\",\n",
    "    \"hadden\",\n",
    "    \"hare\",\n",
    "    \"heb\",\n",
    "    \"hebben\",\n",
    "    \"hebt\",\n",
    "    \"hedden\",\n",
    "    \"heeft\",\n",
    "    \"heel\",\n",
    "    \"hem\",\n",
    "    \"hemzelf\",\n",
    "    \"hen\",\n",
    "    \"het\",\n",
    "    \"hetzelfde\",\n",
    "    \"hier\",\n",
    "    \"hierbeneden\",\n",
    "    \"hierboven\",\n",
    "    \"hierin\",\n",
    "    \"hierna\",\n",
    "    \"hierom\",\n",
    "    \"hij\",\n",
    "    \"hijzelf\",\n",
    "    \"hoe\",\n",
    "    \"hoewel\",\n",
    "    \"honderd\",\n",
    "    \"hun\",\n",
    "    \"hunne\",\n",
    "    \"ieder\",\n",
    "    \"iedere\",\n",
    "    \"iedereen\",\n",
    "    \"iemand\",\n",
    "    \"iets\",\n",
    "    \"ik\",\n",
    "    \"ikzelf\",\n",
    "    \"in\",\n",
    "    \"inderdaad\",\n",
    "    \"inmiddels\",\n",
    "    \"intussen\",\n",
    "    \"inzake\",\n",
    "    \"is\",\n",
    "    \"ja\",\n",
    "    \"je\",\n",
    "    \"jezelf\",\n",
    "    \"jij\",\n",
    "    \"jijzelf\",\n",
    "    \"jou\",\n",
    "    \"jouw\",\n",
    "    \"jouwe\",\n",
    "    \"juist\",\n",
    "    \"jullie\",\n",
    "    \"kan\",\n",
    "    \"klaar\",\n",
    "    \"kon\",\n",
    "    \"konden\",\n",
    "    \"krachtens\",\n",
    "    \"kun\",\n",
    "    \"kunnen\",\n",
    "    \"kunt\",\n",
    "    \"laatst\",\n",
    "    \"later\",\n",
    "    \"liever\",\n",
    "    \"lijken\",\n",
    "    \"lijkt\",\n",
    "    \"maak\",\n",
    "    \"maakt\",\n",
    "    \"maakte\",\n",
    "    \"maakten\",\n",
    "    \"maar\",\n",
    "    \"mag\",\n",
    "    \"maken\",\n",
    "    \"me\",\n",
    "    \"meer\",\n",
    "    \"meest\",\n",
    "    \"meestal\",\n",
    "    \"men\",\n",
    "    \"met\",\n",
    "    \"mevr\",\n",
    "    \"mezelf\",\n",
    "    \"mij\",\n",
    "    \"mijn\",\n",
    "    \"mijnent\",\n",
    "    \"mijner\",\n",
    "    \"mijzelf\",\n",
    "    \"minder\",\n",
    "    \"miss\",\n",
    "    \"misschien\",\n",
    "    \"missen\",\n",
    "    \"mits\",\n",
    "    \"mocht\",\n",
    "    \"mochten\",\n",
    "    \"moest\",\n",
    "    \"moesten\",\n",
    "    \"moet\",\n",
    "    \"moeten\",\n",
    "    \"mogen\",\n",
    "    \"mr\",\n",
    "    \"mrs\",\n",
    "    \"mw\",\n",
    "    \"na\",\n",
    "    \"naar\",\n",
    "    \"nadat\",\n",
    "    \"nam\",\n",
    "    \"namelijk\",\n",
    "    \"nee\",\n",
    "    \"neem\",\n",
    "    \"negen\",\n",
    "    \"nemen\",\n",
    "    \"nergens\",\n",
    "    \"net\",\n",
    "    \"niemand\",\n",
    "    \"niet\",\n",
    "    \"niets\",\n",
    "    \"niks\",\n",
    "    \"noch\",\n",
    "    \"nochtans\",\n",
    "    \"nog\",\n",
    "    \"nogal\",\n",
    "    \"nooit\",\n",
    "    \"nu\",\n",
    "    \"nv\",\n",
    "    \"of\",\n",
    "    \"ofschoon\",\n",
    "    \"om\",\n",
    "    \"omdat\",\n",
    "    \"omhoog\",\n",
    "    \"omlaag\",\n",
    "    \"omstreeks\",\n",
    "    \"omtrent\",\n",
    "    \"omver\",\n",
    "    \"ondanks\",\n",
    "    \"onder\",\n",
    "    \"ondertussen\",\n",
    "    \"ongeveer\",\n",
    "    \"ons\",\n",
    "    \"onszelf\",\n",
    "    \"onze\",\n",
    "    \"onzeker\",\n",
    "    \"ooit\",\n",
    "    \"ook\",\n",
    "    \"op\",\n",
    "    \"opnieuw\",\n",
    "    \"opzij\",\n",
    "    \"over\",\n",
    "    \"overal\",\n",
    "    \"overeind\",\n",
    "    \"overige\",\n",
    "    \"overigens\",\n",
    "    \"paar\",\n",
    "    \"pas\",\n",
    "    \"per\",\n",
    "    \"precies\",\n",
    "    \"recent\",\n",
    "    \"redelijk\",\n",
    "    \"reeds\",\n",
    "    \"rond\",\n",
    "    \"rondom\",\n",
    "    \"samen\",\n",
    "    \"sedert\",\n",
    "    \"sinds\",\n",
    "    \"sindsdien\",\n",
    "    \"slechts\",\n",
    "    \"sommige\",\n",
    "    \"spoedig\",\n",
    "    \"steeds\",\n",
    "    \"tamelijk\",\n",
    "    \"te\",\n",
    "    \"tegen\",\n",
    "    \"tegenover\",\n",
    "    \"tenzij\",\n",
    "    \"terwijl\",\n",
    "    \"thans\",\n",
    "    \"tien\",\n",
    "    \"tiende\",\n",
    "    \"tijdens\",\n",
    "    \"tja\",\n",
    "    \"toch\",\n",
    "    \"toe\",\n",
    "    \"toen\",\n",
    "    \"toenmaals\",\n",
    "    \"toenmalig\",\n",
    "    \"tot\",\n",
    "    \"totdat\",\n",
    "    \"tussen\",\n",
    "    \"twee\",\n",
    "    \"tweede\",\n",
    "    \"u\",\n",
    "    \"uit\",\n",
    "    \"uitgezonderd\",\n",
    "    \"uw\",\n",
    "    \"vaak\",\n",
    "    \"vaakwat\",\n",
    "    \"van\",\n",
    "    \"vanaf\",\n",
    "    \"vandaan\",\n",
    "    \"vanuit\",\n",
    "    \"vanwege\",\n",
    "    \"veel\",\n",
    "    \"veeleer\",\n",
    "    \"veertig\",\n",
    "    \"verder\",\n",
    "    \"verscheidene\",\n",
    "    \"verschillende\",\n",
    "    \"vervolgens\",\n",
    "    \"via\",\n",
    "    \"vier\",\n",
    "    \"vierde\",\n",
    "    \"vijf\",\n",
    "    \"vijfde\",\n",
    "    \"vijftig\",\n",
    "    \"vol\",\n",
    "    \"volgend\",\n",
    "    \"volgens\",\n",
    "    \"voor\",\n",
    "    \"vooraf\",\n",
    "    \"vooral\",\n",
    "    \"vooralsnog\",\n",
    "    \"voorbij\",\n",
    "    \"voordat\",\n",
    "    \"voordezen\",\n",
    "    \"voordien\",\n",
    "    \"voorheen\",\n",
    "    \"voorop\",\n",
    "    \"voorts\",\n",
    "    \"vooruit\",\n",
    "    \"vrij\",\n",
    "    \"vroeg\",\n",
    "    \"waar\",\n",
    "    \"waarom\",\n",
    "    \"waarschijnlijk\",\n",
    "    \"wanneer\",\n",
    "    \"want\",\n",
    "    \"waren\",\n",
    "    \"was\",\n",
    "    \"wat\",\n",
    "    \"we\",\n",
    "    \"wederom\",\n",
    "    \"weer\",\n",
    "    \"weg\",\n",
    "    \"wegens\",\n",
    "    \"weinig\",\n",
    "    \"wel\",\n",
    "    \"weldra\",\n",
    "    \"welk\",\n",
    "    \"welke\",\n",
    "    \"werd\",\n",
    "    \"werden\",\n",
    "    \"werder\",\n",
    "    \"wezen\",\n",
    "    \"whatever\",\n",
    "    \"wie\",\n",
    "    \"wiens\",\n",
    "    \"wier\",\n",
    "    \"wij\",\n",
    "    \"wijzelf\",\n",
    "    \"wil\",\n",
    "    \"wilden\",\n",
    "    \"willen\",\n",
    "    \"word\",\n",
    "    \"worden\",\n",
    "    \"wordt\",\n",
    "    \"zal\",\n",
    "    \"ze\",\n",
    "    \"zei\",\n",
    "    \"zeker\",\n",
    "    \"zelf\",\n",
    "    \"zelfde\",\n",
    "    \"zelfs\",\n",
    "    \"zes\",\n",
    "    \"zeven\",\n",
    "    \"zich\",\n",
    "    \"zichzelf\",\n",
    "    \"zij\",\n",
    "    \"zijn\",\n",
    "    \"zijne\",\n",
    "    \"zijzelf\",\n",
    "    \"zo\",\n",
    "    \"zoals\",\n",
    "    \"zodat\",\n",
    "    \"zodra\",\n",
    "    \"zonder\",\n",
    "    \"zou\",\n",
    "    \"zouden\",\n",
    "    \"zowat\",\n",
    "    \"zulk\",\n",
    "    \"zulke\",\n",
    "    \"zullen\",\n",
    "    \"zult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a94641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/abril/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import DutchStemmer\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "\n",
    "# Loading all users interaction data\n",
    "full_data = pd.read_csv(\"../full_data.csv\")\n",
    "\n",
    "# Loading all content data\n",
    "content = pd.read_csv(\"../content_clean_new.csv\")\n",
    "\n",
    "# The descriptions of our dataset are in Dutch, so we need to use the stopwords in Dutch\n",
    "nltk.download(\"stopwords\")\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "dutch_stopwords.extend(STOPWORDS)\n",
    "dutch_stopwords = set(dutch_stopwords)\n",
    "\n",
    "K_VALUES = {0: 3, 0.5: 5, 1: 7}\n",
    "\n",
    "def clean_text(text):\n",
    "  # Convert to lowercase\n",
    "  text = text.lower()\n",
    "  # Remove punctuation\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  return text\n",
    "\n",
    "\n",
    "def get_cosine_similarity(df, last_seen):\n",
    "  # Descriptions to compare\n",
    "  df = df.copy()\n",
    "\n",
    "  stemmer = DutchStemmer()\n",
    "  df['processed_description'] = df['Description'].apply(lambda x: ' '.join([stemmer.stem(word) for word in clean_text(x).split() if word not in dutch_stopwords]))\n",
    "\n",
    "  # Calculate cosine similarity\n",
    "  vectorizer = TfidfVectorizer(max_df=0.4, min_df=20)\n",
    "  tfidf_matrix = vectorizer.fit_transform(df['processed_description'])\n",
    "\n",
    "  selected_row_index = df[df['item_id'] == last_seen].index[0]\n",
    "\n",
    "  df['cosine_similarity'] = cosine_similarity(tfidf_matrix[selected_row_index], tfidf_matrix).flatten()\n",
    "  \n",
    "  return df\n",
    "\n",
    "# Get set from series tags\n",
    "def get_tag_sets(series):\n",
    "  return series.str.replace(\"[\\]\\']\", \"\", regex=True).str.replace(\" \", \"\").str.split(\",\").apply(lambda x: set(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Get jaccard similarity\n",
    "def get_jaccard_similarity(df, last_seen):\n",
    "  df = df.copy()\n",
    "  df[\"sets\"] = get_tag_sets(df[\"Tags\"])\n",
    "  \n",
    "  # Tags to compare\n",
    "  tags = df.loc[df[\"item_id\"]== last_seen, \"sets\"].iloc[0]\n",
    "  df[\"jaccard_similarity\"] = df[\"sets\"].apply(lambda x: len(x.intersection(tags))/ len(x.union(tags)))\n",
    "  \n",
    "  return df\n",
    "\n",
    "# Normalize a dataframe by subtracting the mean value\n",
    "def norm(dfx):\n",
    "  dfx[\"mean\"] = dfx.mean(axis = 1)\n",
    "  norm = dfx.sub(dfx[\"mean\"], axis = 0)\n",
    "  norm.drop(\"mean\", axis = 1, inplace = True)\n",
    "  return (norm)\n",
    "\n",
    "def track_format(df):\n",
    "  recs = [{\n",
    "      \"image\": row[\"Small_Image\"],\n",
    "      \"large_image\": row[\"Large_Image\"],\n",
    "      \"item_id\": row[\"item_id\"],\n",
    "      \"title\": row[\"Name\"],\n",
    "      \"description\":  row[\"Description\"],\n",
    "      \"tags\": row[\"Tags\"],\n",
    "      \"category\": row[\"Type\"]} for _, row in df.iterrows()]\n",
    "  return recs\n",
    "\n",
    "# Tag list counter function\n",
    "def type_counter(df_series, colab_recomend_df): \n",
    "  # Count the number of items by type and then order the colaborative recommendation list from \"the most\"  favourite \n",
    "  # content type to the least favourite content type\n",
    "  top_tags = df_series.value_counts().sort_values(ascending=False)\n",
    "  cat_order = pd.CategoricalDtype(categories=top_tags.index.tolist(), ordered=True)\n",
    "  # Set \"Type\" column as categorical with defined order\n",
    "  colab_recomend_df['Type'] = colab_recomend_df['Type'].astype(cat_order)\n",
    "  # Sort dataframe by \"Type\" column\n",
    "  return colab_recomend_df.sort_values('Type')\n",
    "\n",
    "\n",
    "# Tag list counter function\n",
    "def tag_counter(df_series, colab_recomend_df):\n",
    "  colab_recomend_df = colab_recomend_df.copy()\n",
    "  # Create a list of all tags in the dataframe\n",
    "  tags = []\n",
    "  for _, row in df_series.iteritems():\n",
    "    tags.extend(eval(row))\n",
    "\n",
    "  # Count the frequency of each tag\n",
    "  tag_counts = pd.Series(tags).value_counts()\n",
    "  tag_list = tag_counts.index.to_list()\n",
    "  \n",
    "  # create a weight variable that decreases as you move down the tag list\n",
    "  tag_weight = {tag_list[i]: len(tag_list) - i for i in range(len(tag_list))}\n",
    "\n",
    "  # count the number of common tags in each row and weight them\n",
    "  colab_recomend_df['Tags'] = colab_recomend_df['Tags'].apply(lambda x: eval(x))\n",
    "  colab_recomend_df['common_tags_weighted'] = colab_recomend_df['Tags'].apply(lambda x: sum([tag_weight[tag] for tag in x if tag in tag_list])/len(x))\n",
    "\n",
    "  # sort the dataframe by the weighted number of common tags in descending order\n",
    "  colab_recomend_df = colab_recomend_df.sort_values('common_tags_weighted', ascending=False)\n",
    "  return colab_recomend_df\n",
    "\n",
    "def collaborative_filtering(user_data, user_data_not_norm, user_id, diversity_level):\n",
    "  k = K_VALUES.get(diversity_level, 3)\n",
    "\n",
    "  target_user_info = user_data.iloc[user_id].values\n",
    "  distances = cosine_similarity(user_data, [target_user_info]).flatten()\n",
    "  distances_with_indices = list(enumerate(distances))\n",
    "  distances_with_indices.sort(key=lambda x: x[1], reverse=False)\n",
    "  top_k_indices = [i for i, _ in distances_with_indices[1:k+1]]\n",
    "\n",
    "  # Get list of items that the target user has not viewed\n",
    "  target_user_row = user_data_not_norm[\"view\"].iloc[user_id]\n",
    "  items_to_rate = target_user_row[target_user_row == 0].index\n",
    "\n",
    "  # Calculate mean interaction score for not viewed items from the group of similar users\n",
    "  item_ratings = []\n",
    "  for item in items_to_rate:\n",
    "    ratings = []\n",
    "    for user_id in top_k_indices:\n",
    "      user_row = user_data_not_norm.iloc[user_id]\n",
    "      rating1 = user_row[\"rating\"][item]\n",
    "      rating2 = user_row[\"shared\"][item]\n",
    "      rating3 = user_row[\"prev\"][item]\n",
    "      ratings.extend([rating1, rating2, rating3])\n",
    "    \n",
    "    ratings = [r for r in ratings if r > 0]\n",
    "    if ratings:\n",
    "      mean = sum(ratings) / len(ratings)\n",
    "      item_ratings.append((item, mean))\n",
    "    \n",
    "  # Sort items by mean rating and return top recommendations\n",
    "  item_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "  collab_recommendations = [item for item, _ in item_ratings[:]]\n",
    "\n",
    "  return pd.DataFrame(collab_recommendations, columns=[\"item_id\"])\n",
    "\n",
    "\n",
    "# Content based recommendations\n",
    "def content_based_filter(colab_recomend_df, user_data_not_norm, user_id):\n",
    "  # Get list of positive interacted content\n",
    "  target_user_row = user_data_not_norm.iloc[user_id]\n",
    "  cols_with_1 = target_user_row[target_user_row == 1].index\n",
    "  tags_rated_pos = [int(re.findall('\\d+', str(col))[0]) for col in cols_with_1]\n",
    "  positive_df = pd.DataFrame(set(tags_rated_pos), columns=[\"item_id\"])\n",
    "\n",
    "  # Data content enrichment\n",
    "  colab_recomend_df = colab_recomend_df.merge(content, how = \"inner\", on = \"item_id\")\n",
    "  positive_df = positive_df.merge(content, how = \"inner\", on = \"item_id\")\n",
    "  \n",
    "  colab_recomend_df = type_counter(positive_df[\"Type\"], colab_recomend_df)\n",
    "  \n",
    "  colab_recomend_df = tag_counter(positive_df[\"Tags\"], colab_recomend_df)\n",
    "\n",
    "  return colab_recomend_df.sort_values([\"Type\", \"common_tags_weighted\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7c7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make all tags a set\n",
    "def make_set(recom_list):\n",
    "  for element in recom_list:\n",
    "    element[\"tags\"] = set(element[\"tags\"])\n",
    "  return recom_list\n",
    "\n",
    "\n",
    "#Function to calculate similarity between tags\n",
    "def jaccard_sim(ele_1, ele_2):\n",
    "  return len(ele_1[\"tags\"].intersection(ele_2[\"tags\"]))/ len(ele_1[\"tags\"].union(ele_2[\"tags\"]))\n",
    "\n",
    "\n",
    "#Function to calculate similarity between types\n",
    "def type_sim(ele_1, ele_2):\n",
    "  return ele_1[\"category\"]==ele_2[\"category\"]\n",
    "\n",
    "\n",
    "#Function to calculate similarity between desxcriptions\n",
    "def cosine_description(ele_1,ele_2):\n",
    "  df = pd.DataFrame({'Description': [ele_1[\"description\"], ele_2[\"description\"]]})\n",
    "  \n",
    "  stemmer = DutchStemmer()\n",
    "  df['processed_description'] = df['Description'].apply(lambda x: ' '.join([stemmer.stem(word) for word in clean_text(x).split() if word not in dutch_stopwords]))\n",
    "\n",
    "  # Calculate cosine similarity\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  tfidf_matrix = vectorizer.fit_transform(df['processed_description'])\n",
    "    \n",
    "  # Calculating cosine similarity\n",
    "  df['cosine_similarity'] = cosine_similarity(tfidf_matrix[0], tfidf_matrix).flatten()\n",
    "\n",
    "  cosine_sim = df.loc[1,'cosine_similarity'] / 0.1\n",
    "  cosine_sim = 1 if cosine_sim > 1 else cosine_sim\n",
    "\n",
    "  return cosine_sim\n",
    "\n",
    "\n",
    "def collaborative_filtering_relevance(user_data, user_data_not_norm, user_id):\n",
    "  k = 5 # top 5 similar users to get ideas of posible relevant items\n",
    "\n",
    "  target_user_info = user_data.iloc[user_id].values\n",
    "  distances = cosine_similarity(user_data, [target_user_info]).flatten()\n",
    "  distances_with_indices = list(enumerate(distances))\n",
    "  distances_with_indices.sort(key=lambda x: x[1], reverse=False)\n",
    "  top_k_indices = [i for i, _ in distances_with_indices[1:k+1]]\n",
    "\n",
    "  # Get list of items that the target user has not viewed\n",
    "  target_user_row = user_data_not_norm[\"view\"].iloc[user_id]\n",
    "  items_to_rate = target_user_row.index\n",
    "\n",
    "  # Calculate mean interaction score for not viewed items from the group of similar users\n",
    "  item_ratings = []\n",
    "  for item in items_to_rate:\n",
    "    ratings = []\n",
    "    for user_id in top_k_indices:\n",
    "      user_row = user_data_not_norm.iloc[user_id]\n",
    "      rating1 = user_row[\"rating\"][item]\n",
    "      rating2 = user_row[\"shared\"][item]\n",
    "      rating3 = user_row[\"prev\"][item]\n",
    "      ratings.extend([rating1, rating2, rating3])\n",
    "    \n",
    "    \n",
    "    if ratings:\n",
    "      mean = sum(ratings) / len(ratings)\n",
    "      item_ratings.append((item, mean))\n",
    "    \n",
    "  # Sort items by mean rating and return top recommendations\n",
    "  item_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "  collab_recommendations = [item for item, _ in item_ratings[:]]\n",
    "\n",
    "  return pd.DataFrame(collab_recommendations, columns=[\"item_id\"])\n",
    "\n",
    "\n",
    "#Function to get revelance of an element for the user\n",
    "def get_relevance_list(id):\n",
    "  # Finding interactions by user id\n",
    "  interactions = list(find_all_interactions_history({\"user_id\": id}))\n",
    "\n",
    "  # Preparing DataFrame\n",
    "  df = pd.pivot_table(full_data, index = \"user_id\", columns = \"item_id\").fillna(0)\n",
    "\n",
    "  # Transform result to DataFrame\n",
    "  if interactions:\n",
    "    interactions = pd.DataFrame(interactions).drop(\"_id\", axis = 1) \n",
    "    interactions = interactions.merge(content, on = \"item_id\")\n",
    "\n",
    "    for _, row in interactions.iterrows():\n",
    "      if row[\"type\"] == \"play\":\n",
    "        df[\"view\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"review\":\n",
    "        df[\"rating\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"share\":\n",
    "        df[\"shared\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"preview\":\n",
    "        df[\"prev\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "\n",
    "  # Normalizing ratings, shares and previews\n",
    "  rating_norm = norm(df[\"rating\"])\n",
    "  shared_norm = norm(df[\"shared\"])\n",
    "  prev_norm = norm(df[\"prev\"])\n",
    "\n",
    "  # Union of all datasets\n",
    "  user_data = pd.concat([df[\"view\"], rating_norm, shared_norm, prev_norm], axis = 1)\n",
    "\n",
    "  # Collaborative filtering part\n",
    "  colab_recomend_df = collaborative_filtering_relevance(user_data, df, id)\n",
    "\n",
    "  # Content based filtering part\n",
    "  content_recomend_df = content_based_filter(colab_recomend_df, df, id)\n",
    "        \n",
    "  #Get the position of the item id\n",
    "  content_recomend_df.reset_index(inplace = True, drop = True)\n",
    "      \n",
    "  return content_recomend_df\n",
    "        \n",
    "     \n",
    "def get_relevance(recoms, rel_list):\n",
    "  relevant_count = 0\n",
    "  for i in recoms:\n",
    "    pos =  rel_list[rel_list['item_id']== i['item_id']].index.values.astype(int)[0]\n",
    "    prob = (len(rel_list)-pos) / (len(rel_list)-1)\n",
    "    \n",
    "    #Relevant or not relevant item with prob\n",
    "    rel = choices([0, 1], weights=(1-prob, prob),k=1)[0]\n",
    "    relevant_count = relevant_count + rel\n",
    "    \n",
    "  # Calculate list avg rel\n",
    "  return relevant_count/len(recoms)\n",
    "\n",
    "\n",
    "#Defining primitive preditor function\n",
    "def get_primitive():\n",
    "  # Preparing DataFrame\n",
    "  df = full_data.copy()\n",
    "\n",
    "  # Order content by number of views and rating\n",
    "  grouped_df = df.groupby('item_id').agg({'view': ['sum'], 'rating': ['mean']})\n",
    "  C = grouped_df[('rating', 'mean')].mean()\n",
    "  m = grouped_df[('view', 'sum')].quantile(0.70)\n",
    "\n",
    "  R = grouped_df[('rating', 'mean')].values\n",
    "  v = grouped_df[('view', 'sum')].values\n",
    "  weights = (R + C) / (v + m)\n",
    "\n",
    "  grouped_df[\"weight\"] = weights\n",
    "\n",
    "  grouped_df = grouped_df.sort_values(\"weight\", ascending=False)\n",
    "\n",
    "  # Get content information to return recommendations to front\n",
    "  grouped_df = grouped_df.merge(content, how = \"inner\", on = \"item_id\")\n",
    "  normal_recommendations = track_format(grouped_df.head(25))\n",
    "\n",
    "  prim_list = []\n",
    "  for i in normal_recommendations:\n",
    "    prim_list.append(i[\"item_id\"])\n",
    "  return prim_list\n",
    "\n",
    "\n",
    "def get_expected_items(id):    \n",
    "  # Finding interactions by user id\n",
    "  interactions = list(find_all_interactions_history({\"user_id\": id}))\n",
    "\n",
    "  # Preparing DataFrame\n",
    "  df = pd.pivot_table(full_data, index = \"user_id\", columns = \"item_id\").fillna(0)\n",
    "\n",
    "  # Transform result to DataFrame\n",
    "  if interactions:\n",
    "    interactions = pd.DataFrame(interactions).drop(\"_id\", axis = 1) \n",
    "    interactions = interactions.merge(content, on = \"item_id\")\n",
    "    \n",
    "    for _, row in interactions.iterrows():\n",
    "      if row[\"type\"] == \"play\":\n",
    "        df[\"view\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"review\":\n",
    "        df[\"rating\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"share\":\n",
    "        df[\"shared\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"preview\":\n",
    "        df[\"prev\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "\n",
    " \n",
    "  # Get list of positive interacted content (expectd items)\n",
    "  target_user_row = df.iloc[id]\n",
    "  cols_with_1 = target_user_row[target_user_row == 1].index\n",
    "  tags_rated_pos = [int(re.findall('\\d+', str(col))[0]) for col in cols_with_1]\n",
    "  positive_items = set(tags_rated_pos)\n",
    "  \n",
    "  # List to append all the known and expectec content\n",
    "  list_expected = []\n",
    "  \n",
    "  # Get the closest content to user interactions\n",
    "  for item in positive_items:\n",
    "      # Get jaccard distance between tags\n",
    "      content_dis = get_jaccard_similarity(content,item)\n",
    "      \n",
    "      type_item = content.loc[content[\"item_id\"]==item].reset_index()\n",
    "        \n",
    "      # Content type check\n",
    "      content_dis[\"Type_sim\"] = content_dis[\"Type\"] == type_item.loc[0,\"Type\"]\n",
    "    \n",
    "      # Get levshtein distance between descriptions, sorting\n",
    "      content_lev = get_cosine_similarity(content_dis, item).sort_values(by=[\"Type_sim\",\"jaccard_similarity\", \"cosine_similarity\"], ascending = [False, False, False])\n",
    "\n",
    "      # Removing same item  \n",
    "      content_lev = content_lev.loc[content_lev[\"item_id\"] != item]\n",
    "    \n",
    "      # Appending item + 5 most similar item\n",
    "      list_expected.append(item)\n",
    "      for i in range(5):\n",
    "          list_expected.append(content_lev.iloc[i][\"item_id\"])\n",
    "  \n",
    "  #Returning list with expected items\n",
    "  return list_expected\n",
    "    \n",
    "\n",
    "#Function to calculate metrics score of each list\n",
    "def get_scores(recoms,id):\n",
    "  list_recommendations = make_set(recoms) \n",
    "  \n",
    "  # Get relevance list\n",
    "  list_rel = get_relevance_list(id)\n",
    "  \n",
    "  #Get relevance of the recomendation list\n",
    "  relevance = get_relevance(recoms,list_rel)\n",
    "    \n",
    "  # Get sum\n",
    "  sum_div = 0\n",
    "    \n",
    "  primitive = set(get_primitive())\n",
    "  # List of expected items\n",
    "  expected = set(get_expected_items(id))\n",
    "    \n",
    "  # Getting recommended ids\n",
    "  rs_list = []\n",
    "  for i in recoms:\n",
    "    rs_list.append(i[\"item_id\"])\n",
    "  rs_set = set(rs_list)\n",
    "  \n",
    "  # Primitive union expected\n",
    "  PE = primitive.union(expected)\n",
    "  \n",
    "  #Getting set diference\n",
    "  unexpected = rs_set.difference(PE)\n",
    "  \n",
    "  # Lenght \n",
    "  n = len(list_recommendations)\n",
    "  for i in range (n):\n",
    "    for j in range (len(list_recommendations)):      \n",
    "      #Define elements to compare\n",
    "      ele_1, ele_2 = list_recommendations[i], list_recommendations[j]\n",
    "            \n",
    "      #Calculate our diversity measure\n",
    "      div = (3 - (jaccard_sim(ele_1, ele_2)+(type_sim(ele_1,ele_2))+(cosine_description(ele_1,ele_2))))\n",
    "      sum_div = sum_div + div\n",
    "    \n",
    "    \n",
    "  #Getting ser of each unexpected item\n",
    "  ser_sum= len(unexpected)  \n",
    "  intra_diversity = sum_div /((n/2)* (n-1))\n",
    "  serindipity = (ser_sum/n) * relevance\n",
    "  return intra_diversity,relevance,serindipity\n",
    "\n",
    "\n",
    "def get_random_rs():\n",
    "  random_list =  shuffle(content).reset_index()\n",
    "  recoms = track_format(random_list)[:15]\n",
    "  return recoms\n",
    "\n",
    "\n",
    "def get_personalised_recommendations(id, diversity_level):\n",
    "  # Finding interactions by user id\n",
    "  interactions = list(find_all_interactions_history({\"user_id\": id}))\n",
    "\n",
    "  # Preparing DataFrame\n",
    "  df = pd.pivot_table(full_data, index = \"user_id\", columns = \"item_id\").fillna(0)\n",
    "\n",
    "  # Transform result to DataFrame\n",
    "  if interactions:\n",
    "    interactions = pd.DataFrame(interactions).drop(\"_id\", axis = 1) \n",
    "    interactions = interactions.merge(content, on = \"item_id\")\n",
    "    \n",
    "    for _, row in interactions.iterrows():\n",
    "      if row[\"type\"] == \"play\":\n",
    "        df[\"view\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"review\":\n",
    "        df[\"rating\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"share\":\n",
    "        df[\"shared\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "      elif row[\"type\"] == \"preview\":\n",
    "        df[\"prev\"].loc[id, row[\"item_id\"]] = row[\"value\"]\n",
    "\n",
    "  # Normalizing ratings\n",
    "  rating_norm = norm(df[\"rating\"])\n",
    "  # Normalizing shares\n",
    "  shared_norm = norm(df[\"shared\"])\n",
    "  # Normalizing previews\n",
    "  prev_norm = norm(df[\"prev\"])\n",
    "\n",
    "  # Union of all datasets\n",
    "  user_data = pd.concat([df[\"view\"], rating_norm, shared_norm, prev_norm], axis = 1)\n",
    "\n",
    "  # Collaborative filtering part\n",
    "  colab_recomend_df = collaborative_filtering(user_data, df, id, diversity_level)\n",
    "\n",
    "  # Content based filtering part\n",
    "  content_recomend_df = content_based_filter(colab_recomend_df, df, id)\n",
    "  \n",
    "  # Giving recommendations the correct format\n",
    "  normal_recomendations = track_format(content_recomend_df)\n",
    "\n",
    "  # Low diversity: List is not modified\n",
    "  if diversity_level == 0:\n",
    "    normal_recomendations = normal_recomendations[:15]\n",
    "  \n",
    "  # Medium diversity: Mix between random and ordered list\n",
    "  elif diversity_level == 0.5:\n",
    "    #Half random, Half not modified\n",
    "    not_random = [0,1,2,3,4,5,6]\n",
    "    yes_random = random.sample(range(8, len(normal_recomendations)), 8 if len(normal_recomendations) > 15 else len(normal_recomendations) - 8)\n",
    "    \n",
    "    list_ran = not_random + yes_random\n",
    "    random.shuffle(list_ran)\n",
    "    normal_recomendations = list(np.array(normal_recomendations)[list_ran])\n",
    "  \n",
    "  # High diversity: Order list randomly\n",
    "  elif diversity_level == 1:\n",
    "    #few not modified, the  rest random\n",
    "    not_random = [0,1,2]\n",
    "    yes_random = random.sample(range(3, len(normal_recomendations)), 12 if len(normal_recomendations) > 15 else len(normal_recomendations) - 3)\n",
    "    \n",
    "    list_ran = not_random + yes_random\n",
    "    random.shuffle(list_ran)\n",
    "    normal_recomendations = list(np.array(normal_recomendations)[list_ran])\n",
    "  \n",
    "  return normal_recomendations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982dd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_users = random.sample(range(1000), 100)\n",
    "\n",
    "#To store vars\n",
    "user = []\n",
    "l_intra_diversity, l_relevance, l_diversity_rel,l_serendipity  = [],[],[],[]\n",
    "m_intra_diversity, m_relevance, m_diversity_rel,m_serendipity  = [],[],[],[]\n",
    "h_intra_diversity, h_relevance, h_diversity_rel,h_serendipity  = [],[],[],[]\n",
    "r_intra_diversity, r_relevance, r_diversity_rel,r_serendipity  = [],[],[],[]\n",
    "\n",
    "for id in list_users:\n",
    "    user.append(id)\n",
    "\n",
    "    recoms = get_personalised_recommendations(id, 0)\n",
    "    scores = get_scores(recoms,id)\n",
    "    # Appending results\n",
    "    l_intra_diversity.append(scores[0])\n",
    "    l_relevance.append(scores[1])\n",
    "    l_diversity_rel.append(scores[0]*scores[1])\n",
    "    l_serendipity.append(scores[2])\n",
    "   \n",
    "    recoms = get_personalised_recommendations(id, 0.5)\n",
    "    scores = get_scores(recoms,id)\n",
    "    # Appending results\n",
    "    m_intra_diversity.append(scores[0])\n",
    "    m_relevance.append(scores[1])\n",
    "    m_diversity_rel.append(scores[0]*scores[1])\n",
    "    m_serendipity.append(scores[2])\n",
    "   \n",
    "\n",
    "    recoms = get_personalised_recommendations(id, 1)\n",
    "    scores = get_scores(recoms,id)\n",
    "    #Appending results\n",
    "    h_intra_diversity.append(scores[0])\n",
    "    h_relevance.append(scores[1])\n",
    "    h_diversity_rel.append(scores[0]*scores[1])\n",
    "    h_serendipity.append(scores[2])\n",
    "   \n",
    "    recoms = get_random_rs()\n",
    "    scores = get_scores(recoms,id)\n",
    "    #Appending results\n",
    "    r_intra_diversity.append(scores[0])\n",
    "    r_relevance.append(scores[1])\n",
    "    r_diversity_rel.append(scores[0]*scores[1])\n",
    "    r_serendipity.append(scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"user_id\": user, \"low_intralist_diversity\":l_intra_diversity, \"med_intralist_diversity\":m_intra_diversity, \"high_intralist_diversity\":h_intra_diversity, \"random_intralist_diversity\":r_intra_diversity, \"low_relevance\":l_relevance,\"med_relevance\":m_relevance , \"high_relevance\":h_relevance, \"random_relevance\":r_relevance, \"low_diversity*relevance\":l_diversity_rel, \"med_diversity*relevance\":m_diversity_rel, \"high_diversity*relevance\":h_diversity_rel, \"random_diversity*relevance\":r_diversity_rel ,\"low_serendipity\":l_serendipity, \"med_serendipity\":m_serendipity, \"high_serendipity\":h_serendipity, \"random_serendipity\":r_serendipity } )\n",
    "results.to_csv(\"results.csv\", index= 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
