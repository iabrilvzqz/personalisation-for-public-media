{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function provides a random user agent string to use for a web scraping session, \n",
    "helping to reduce the chances of being blocked by a website server.\"\"\"\n",
    "\n",
    "def get_new_useragent():\n",
    "    list_agents = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"]\n",
    "    USER_AGENT = random.choice(list_agents)\n",
    "    return {\"user-agent\": USER_AGENT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is a Python function that performs web scraping on the website of the Dutch public broadcasting organization, \n",
    "NPO, to extract information about content in a specified category.\"\"\"\n",
    "\n",
    "def scrap_npo(category, keys):\n",
    "    results = pd.DataFrame(columns = [\"Name\", \"Description\", \"Category\", \"URL\", \"Small_Image\", \"Large_Image\"])\n",
    "\n",
    "    # Set the options for the Chrome webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "    # Start the Chrome webdriver\n",
    "    driver = webdriver.Chrome('/Users/abril/Documents/Utrecht_University/Block_3/Personalisation_for_Public_Media/Final_Project/back/scrapping/chromedriver', options=options)\n",
    "\n",
    "    # Navigate to the webpage with the list of TV shows\n",
    "    driver.get(keys[\"url\"])\n",
    "\n",
    "    # Handle cookie consent box (if it exists)\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        cookie_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#ccm_close\")))\n",
    "        cookie_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Wait for the \"load more\" button to be available\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    load_more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".load-more-button\")))\n",
    "\n",
    "    # Click the \"load more\" button until it disappears\n",
    "    while load_more_button.is_displayed():\n",
    "        try:\n",
    "            load_more_button.click()\n",
    "            load_more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".load-more-button\")))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Find the parent div that contains the list of all TV shows\n",
    "    parent_div = soup.find(\"div\", {\"class\": keys[\"classes\"][\"parent_div\"]})\n",
    "\n",
    "    # Find all the child divs that contain the TV shows\n",
    "    tvshow_divs = parent_div.find_all(\"div\", {\"class\": keys[\"classes\"][\"tile-container\"]})\n",
    "\n",
    "    # Loop through each TV show in the list and extract its name, image, and URL\n",
    "    for tvshow in tvshow_divs:\n",
    "        url = tvshow.find(\"a\")[\"href\"]\n",
    "        name = tvshow.find(\"div\", {\"class\": keys[\"classes\"][\"tile-title\"]}).text.strip()\n",
    "        small_image = tvshow.find(\"div\", {\"class\": keys[\"classes\"][\"tile-image\"]}).find(\"img\")[\"src\"]\n",
    "\n",
    "        # Send a GET request to the TV show's URL\n",
    "        time.sleep(2)\n",
    "        headers = get_new_useragent() # Adding a random user agent\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Parse the HTML content of the TV show's page using BeautifulSoup\n",
    "        soup2 = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        \"\"\" The description and image of a show are included in the meta tags of a web page because \n",
    "        they provide useful information to search engines and social media platform. Including this information\n",
    "        help to improve the visibility of the webpage and engagement on both search engines and social media platforms.\"\"\"\n",
    "        \n",
    "        # Find the description of the item in the meta tag that contains the description\n",
    "        try: description = soup2.find(\"meta\", property=\"description\")[\"content\"]\n",
    "        except: description = \"\"\n",
    "\n",
    "        # Find the image of the item in the meta tag\n",
    "        try: background_image_url = soup2.find(\"meta\", property=\"og:image\")[\"content\"]\n",
    "        except: background_image_url = small_image\n",
    "\n",
    "        new_result = pd.DataFrame({\"Name\" : name, \"Description\" : description, \"Category\": category, \"URL\" : url, \"Small_Image\": small_image, \"Large_Image\": background_image_url}, index=[0])\n",
    "        results = pd.concat([new_result, results.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "    # Close the browsers\n",
    "    driver.quit()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters that we need to webscrap different pages in the NPOStart Platform\n",
    "# The classes change in the programmas section, that's why we need to differentiate them\n",
    "programmas_classes = {\n",
    "    \"parent_div\": \"npo-grid-teaser\",\n",
    "    \"tile-container\": \"npo-ankeiler-tile-container\",\n",
    "    \"tile-image\": \"npo-ankeiler-tile-image\",\n",
    "    \"tile-title\": \"npo-ankeiler-tile-image\"\n",
    "}\n",
    "\n",
    "other_classes = {\n",
    "    \"parent_div\": \"npo-grid-asset\",\n",
    "    \"tile-container\": \"npo-asset-tile-container\",\n",
    "    \"tile-image\": \"npo-asset-tile-image\",\n",
    "    \"tile-title\": \"npo-asset-tile-title\"\n",
    "}\n",
    "\n",
    "# For each category the section of the webpage that we want to scrap changes, that's why we are including different urls in this dictionary\n",
    "categories = {\n",
    "    \"Programmas\": {\"url\": \"https://www.npostart.nl/programmas\", \"classes\": programmas_classes},\n",
    "    \"Series\": {\"url\": \"https://www.npostart.nl/collectie/POMS_S_NPO_3712010\", \"classes\": other_classes},\n",
    "    \"Documentaires\": {\"url\": \"https://www.npostart.nl/collectie/POMS_S_NPO_8166639\", \"classes\": other_classes},\n",
    "    \"Films\": {\"url\": \"https://www.npostart.nl/collectie/POMS_S_NPO_8282004\", \"classes\": other_classes}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/xfklpqfj6kjg8yr6l1byph0h0000gn/T/ipykernel_19372/4258802763.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/Users/abril/Documents/Utrecht_University/Block_3/Personalisation_for_Public_Media/Final_Project/back/scrapping/chromedriver', options=options)\n"
     ]
    }
   ],
   "source": [
    "for key, value in categories.items():\n",
    "    result = scrap_npo(key, value)\n",
    "    result.to_csv(\"{}.csv\".format(key))\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function computes the similarity ratio between two strings a and b using the SequenceMatcher class \n",
    "from the difflib module in Python. The SequenceMatcher class takes two sequences and returns a measure of their similarity \n",
    "as a ratio between 0 and 1, with 0 indicating no similarity between the two strings, and 1 indicating that the two strings \n",
    "are identical.\"\"\"\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, str(a), str(b)).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is a Python function named get_more_info that scrapes information about TV shows from the Movie Meter website. \n",
    "The function takes a row as an argument, which is a Pandas dataframe row that contains information about a TV show such as \n",
    "its name, description, small image and large image. We will use this function to complete the information that could not be\n",
    "found in the NPO website.\"\"\"\n",
    "\n",
    "def get_more_info(row, driver):\n",
    "    # Set the options for the Chrome webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "    # Navigate to Movie Meter search page\n",
    "    driver.get(\"https://www.moviemeter.nl/site/zoeken/{}\".format(row[\"Name\"]))\n",
    "\n",
    "    # Wait for the results to load\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    # Find the first search result and click on it\n",
    "    try:\n",
    "        first_result = driver.find_element(By.CSS_SELECTOR, \"div.sResults a\")\n",
    "        first_result.click()\n",
    "    except:\n",
    "        return {\"tags\": [], \"description\": row[\"Description\"], \"small_image\": row[\"Small_Image\"], \"large_image\": row[\"Large_Image\"]}\n",
    "    \n",
    "    # If we continue it's because we got results in the search website\n",
    "    # Send a GET request to the TV show's URL\n",
    "    time.sleep(1)\n",
    "    headers = get_new_useragent() # adding a random user agent\n",
    "    response = requests.get(driver.current_url, headers=headers)\n",
    "\n",
    "    # Parse the HTML content of the TV show's page using BeautifulSoup\n",
    "    soup2 = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Get title of the webpage and check how similar it is from the title that we are trying to search\n",
    "    try:\n",
    "        title = soup2.find(\"div\", {\"class\": \"title\"}).find(\"h1\").text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "    similarity = similar(title, row[\"Name\"])\n",
    "\n",
    "    # If the found title and the show we are searching are very similar, we perform the webscrapping\n",
    "    if similarity >= 0.65:\n",
    "        # Find the tags in the HTML using a CSS selector\n",
    "        try: tags = soup2.find(\"div\", {\"class\": \"genre_holder\"}).find_all(\"a\")\n",
    "        except: tags = []\n",
    "\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            result.append(tag.text.strip())\n",
    "\n",
    "        if pd.isna(row[\"Description\"]):\n",
    "            description_div = soup2.find(\"div\", {\"class\": \"blog-bar\"}).find(\"p\")\n",
    "            if description_div:\n",
    "                description = description_div.text.strip()\n",
    "            else:\n",
    "                description = row[\"Description\"]\n",
    "        else: description = row[\"Description\"]\n",
    "\n",
    "        # I we didn't find a small image in the NPO website, we will try to find one in the movie meter platform\n",
    "        if not row[\"Small_Image\"].startswith(\"https://\"):\n",
    "            small_image = soup2.find(\"div\", {\"class\": \"figure\"}).find(\"img\")[\"src\"]\n",
    "        else: small_image = row[\"Small_Image\"]\n",
    "    \n",
    "        if not row[\"Large_Image\"].startswith(\"https://\"):\n",
    "            # Find the div containing the background image using a CSS selector\n",
    "            div = driver.find_element(By.CSS_SELECTOR, \"div.film-header-bar\")\n",
    "    \n",
    "            # Get the background image URL using JavaScript\n",
    "            background_image = driver.execute_script(\"return getComputedStyle(arguments[0]).backgroundImage;\", div)\n",
    "            # Define the regular expression pattern\n",
    "            pattern = r\"url\\(\\\"(.*)\\\"\\)\"\n",
    "\n",
    "            # Find the URL using the regular expression\n",
    "            match = re.search(pattern, background_image)\n",
    "\n",
    "            # Extract the URL from the match object\n",
    "            background_image_url = match.group(1)\n",
    "        else:\n",
    "            background_image_url = small_image\n",
    "    else:\n",
    "        return {\"tags\": [], \"description\": row[\"Description\"], \"small_image\": row[\"Small_Image\"], \"large_image\": row[\"Large_Image\"]}\n",
    "\n",
    "    return {\"tags\": result, \"description\": description, \"small_image\": small_image, \"large_image\": background_image_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/xfklpqfj6kjg8yr6l1byph0h0000gn/T/ipykernel_2474/1141833779.py:13: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/Users/abril/Documents/Utrecht_University/Block_3/Personalisation_for_Public_Media/Final_Project/back/scrapping/chromedriver', options=options)\n"
     ]
    }
   ],
   "source": [
    "# Set the options for the Chrome webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "\n",
    "for key in categories:\n",
    "    df = pd.read_csv(\"{}.csv\".format(key))\n",
    "    df = df.drop_duplicates(subset=[\"Name\"], keep=\"last\")\n",
    "    df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "    df[\"Tags\"] = \"\"\n",
    "\n",
    "    # Start the Chrome webdriver\n",
    "    driver = webdriver.Chrome('/Users/abril/Documents/Utrecht_University/Block_3/Personalisation_for_Public_Media/Final_Project/back/scrapping/chromedriver', options=options)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        results = get_more_info(row, driver)\n",
    "        df.at[index, \"Tags\"] = results[\"tags\"]\n",
    "        df.at[index, \"Description\"] = results[\"description\"]\n",
    "        df.at[index, \"Small_Image\"] = results[\"small_image\"]\n",
    "        df.at[index, \"Large_Image\"] = results[\"large_image\"]\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "    df.to_csv(\"{}_complete.csv\".format(key))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infomcdmmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9121169c2462db3065366b7b50db92e9cc720016fa326400dee081043796e520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
